{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "import re\n",
    "\n",
    "file1 = open(\"C://Eday//docs//Datascience//JHU//en_US.blogs.txt\",mode='r',encoding=\"utf-8\")\n",
    "file2 = open(\"C://Eday//docs//Datascience//JHU//en_US.news.txt\",mode='r',encoding=\"utf-8\")\n",
    "file3 = open(\"C://Eday//docs//Datascience//JHU//en_US.twitter.txt\",mode='r',encoding=\"utf-8\")\n",
    "\n",
    "US_blogs = file1.readlines()\n",
    "US_news = file2.readlines()\n",
    "US_twitter = file3.readlines()\n",
    "\n",
    "text = US_blogs[1:1500] + US_news[1:1500] + US_twitter[1:1500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4497"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chad has been awesome with the kids and holding down the fort while I work later than usual! The kids have been busy together playing Skylander on the XBox together, after Kyan cashed in his $$$ from his piggy bank. He wanted that game so bad and used his gift card from his birthday he has been saving and the money to get it (he never taps into that thing either, that is how we know he wanted it so bad). We made him count all of his money to make sure that he had enough! It was very cute to watch his reaction when he realized he did! He also does a very good job of letting Lola feel like she is playing too, by letting her switch out the characters! She loves it almost as much as him.\\n',\n",
       " 'so anyways, i am going to share some home decor inspiration that i have been storing in my folder on the puter. i have all these amazing images stored away ready to come to life when we get our home.\\n',\n",
       " \"With graduation season right around the corner, Nancy has whipped up a fun set to help you out with not only your graduation cards and gifts, but any occasion that brings on a change in one's life. I stamped the images in Memento Tuxedo Black and cut them out with circle Nestabilities. I embossed the kraft and red cardstock with TE's new Stars Impressions Plate, which is double sided and gives you 2 fantastic patterns. You can see how to use the Impressions Plates in this tutorial Taylor created. Just one pass through your die cut machine using the Embossing Pad Kit is all you need to do - super easy!\\n\",\n",
       " \"If you have an alternative argument, let's hear it! :)\\n\",\n",
       " 'If I were a bear,\\n',\n",
       " 'Other friends have similar stories, of how they were treated brusquely by Laurelwood staff, and as often as not, the same names keep coming up. About a half-dozen friends of mine refuse to step foot in there ever again because of it. How many others they’re telling - and keeping away - one can only guess.\\n',\n",
       " 'Although our beloved Cantab can’t claim the international recognition afforded the Station Inn, otherwise these two joints feel like twins separated by nothing more than distance. They share a complete lack of pretense that can’t be imitated or approximated. Their very ordinariness makes them special.\\n',\n",
       " 'Peter Schiff: Hard to tell. It will look pretty bad for most Americans when prices will go way up and they can’t afford to buy stuff. It could also get very bad as far as loss of individual liberty. A lot of people will blame it on capitalism, on freedom, and they will claim we need more government. It could be used as an impetus for more regulation, which would be a disaster, or it could be an impetus to get rid of all the regulation that was causing the problem. But whether we will do the right or the wrong thing here in America, there will be a lot of pain first. We got some serious problems we have to deal with, but we are not dealing with the problems, we only make the problems worse.\\n',\n",
       " '3. Winter is the time when all sleeps on some level. It is a time of turning within to make contact with your own innermost being. Winter is the best time to do this and, as a result, get answers for yourself about whatever concerns you. It is also a tremendously creative time, a time for artists of all kinds to turn within and to seek images of forms he or she wishes to produce in whatever medium chosen, so that an image of balance can be presented. Those of you that write, we suggest that you do so to come up with plots and ideas that will speak of balance coming to an imbalanced situation, person, or world. To those of you who, like Priscilla, work with making patterns and designs, we would say remember the images and designs brought about by winter. White will challenge you to see what changes will manifest in the images you produce. We ask that you do this in your own environment.\\n',\n",
       " 'I am en-route to Cornwall again. 3/4 months of slog and sun and sea too. Always a job needs doing in a tourist town. I’m bringing my stuff back from 97 on Friday. Will need to cancel bills before I retreat. Not such a bad thing. Moved to Leeds with promise from Millies. That went nowhere. Move on.\\n',\n",
       " 'Pure large leaf Assam. No waffling with the leaf, thank you. I want it strong and dark with no herby frills. And for goodness sake no fruit mixers and no sweetener. Why would you do that to tea?\\n',\n",
       " 'The also mention the effect on the night sky, an issue we raised in our report, which again was ignore by Lancing Parish Council!. “The use of all lighting, including that of the indoor football pitch dome, should be the subject of a planning condition to ensure that it is only utilised at appropriate times and when necessary to safeguard the ‘dark skies’ of the adjacent NP.“\\n',\n",
       " 'State contracts worth over 100 MILLION ringgit!\\n',\n",
       " 'People like you are unknowing transformers of things, protected by your own fairy-tale, by love.”\\n',\n",
       " 'The one thing that was astounding though was the support from the marshals – they were all phenomenal for being out in the rain for so long and remaining so cheery and supportive. They were an amazing bunch. Owing to the nature of the course and very few closed roads meant that supporters who knew the area were able to skip around the course and a few people were seen about 5 or 6 times which was also super!!\\n',\n",
       " \"We attend our friend Kendra's wedding -- such a joy to watch people see an end to year's of waiting.\\n\",\n",
       " 'A neighbor recommended “Chasing Fireflies” by Charles Martin. I’d never read anything by Martin before, although he was this neighbor’s favorite author. “Fireflies” is the story of Chase, a journalist who never knew his parents and was adopted by “Unc,” a colorful resident of a small Georgia town whose sayings and kind heart make him a memorable character. Chase is investigating the story of another orphan, this one a mute boy pushed out of a car on train tracks just before a train hit and killed his loser custodial “parent.”\\n',\n",
       " 'Fallout by Ellen Hopkins (p.1-140)\\n',\n",
       " 'Either Obama/Congress, etc. give in or, more likely, refuse. The group announces that the next device will be detonated in a populated area, with no warning. Again, “Obama,” et al., refuse to submit. This time the device is detonated in a mid-sized to major city. Tens of thousands die. (Think the atomic bombs that ended WWII.)\\n']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import contractions\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def spacy_lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def simple_stemming(text, stemmer=ps):\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
    "    if not stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    \n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def text_pre_processor(text, html_strip=True, accented_char_removal=True, contraction_expansion=True,\n",
    "                       text_lower_case=True, text_stemming=False, text_lemmatization=True, \n",
    "                       special_char_removal=True, remove_digits=True, stopword_removal=True, \n",
    "                       stopword_list=None):\n",
    "    \n",
    "    # strip HTML\n",
    "    if html_strip:\n",
    "        text = strip_html_tags(text)\n",
    "    \n",
    "    # remove extra newlines (often might be present in really noisy text)\n",
    "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    \n",
    "    # remove accented characters\n",
    "    if accented_char_removal:\n",
    "        text = remove_accented_chars(text)\n",
    "    \n",
    "    # expand contractions    \n",
    "    if contraction_expansion:\n",
    "        text = expand_contractions(text)\n",
    "        \n",
    "    \n",
    "    # lemmatize text\n",
    "    if text_lemmatization:\n",
    "        text = spacy_lemmatize_text(text) \n",
    "        \n",
    "    # remove special characters and\\or digits    \n",
    "    if special_char_removal:\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        text = special_char_pattern.sub(\" \\\\1 \", text)\n",
    "        text = remove_special_characters(text, remove_digits=remove_digits)  \n",
    "        \n",
    "    # stem text\n",
    "    if text_stemming and not text_lemmatization:\n",
    "        text = simple_stemming(text)\n",
    "        \n",
    "    # lowercase the text    \n",
    "    if text_lower_case:\n",
    "        text = text.lower()\n",
    "        \n",
    "        \n",
    "    # remove stopwords\n",
    "    if stopword_removal:\n",
    "        text = remove_stopwords(text, is_lower_case=text_lower_case, \n",
    "                                stopwords=stopword_list)\n",
    "        \n",
    "    # remove extra whitespace\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def corpus_pre_processor(corpus):\n",
    "    norm_corpus = []\n",
    "    for doc in tqdm.tqdm(corpus):\n",
    "        norm_corpus.append(text_pre_processor(doc))\n",
    "    return norm_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ text_pre_processor(line) for line in text ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4497"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chad awesome kid hold fort work later usual kid busy together play skylander xbox together kyan cash piggy bank want game bad use gift card birthday save money get never tap thing either know want bad make count money make sure enough cute watch reaction realize also good job let lola feel like play let switch character love almost much']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordlist = [\"hi what's up home diddle mc doo\", \"Oh wise master kakarot\",\"Hi Hello\"]\n",
    "data_limit = list(map(lambda x: ' '.join(x.split(' ')[:100]), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4497"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whether look invest residential commercial industrial property urban redevelopment authoritys ura area planning guideline long term impact propertys value',\n",
       " 'thus begin frantic sprint corridor pull reluctant suitcase bob anything path',\n",
       " 'star make magazine sexiest women alive list include model turn actress rosie huntington whiteley jazz artist esperanza spalding friends benefit star mila kunis help actress emma stone singer beyonce',\n",
       " 'ah parga many time sit paxos look across mainland night see twinkling light far distance many time wonder like well know unfortunately place twinkle light parga little community get wrong parga itinerary anyway conclude worth detour two resort bay separate ancient turkish fort steep hill cluster souvenir shop taverna restaurant marina unwelcoming moor alongside rusting capsized ferry trudge buy provision take us along neglect rubbish strew track thomas would leave boat fear rob hey pull gangplank crack open beer experiment cooking galley',\n",
       " 'blakeman ingle']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_limit[50:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chad awesome kid hold fort work later usual kid busy together play skylander xbox together kyan cash piggy bank want game bad use gift card birthday save money get never tap thing either know want bad make count money make sure enough cute watch reaction realize also good job let lola feel like play let switch character love almost much',\n",
       " 'anyways go share home decor inspiration store folder puter amazing image store away ready come life get home',\n",
       " 'graduation season right around corner nancy whip fun set help graduation card gift occasion bring change one life stamp image memento tuxedo black cut circle nestabilities emboss kraft red cardstock te new stars impressions plate double side give fantastic pattern see use impressions plates tutorial taylor create one pass die cut machine use embossing pad kit need super easy']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_red = [ data.split(' ') for line in text ]\n",
    "#res = [][]\n",
    "#res = [ ''.joword for line in data for word in line.split(' ') ]\n",
    "#sub.split()[N - 1] \n",
    "# [j for i in range(100) if i > 10 for j in range(i) if j < 20]\n",
    "#if len(sub.split()) > 1\n",
    "#print(len(res),res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 13657\n",
      "Total Sequences: 445203\n",
      "Max Sequence Length: 100\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "#print(tokenizer.index_word)\n",
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# create line-based sequences\n",
    "sequences = list()\n",
    "#for line in data.split('\\n'):\n",
    "for line in text:\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\t#for i in range(1, len(encoded)):\n",
    "\tfor i in range(1, 100):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "#print(sequences)\n",
    "# pad input sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "#sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "sequences = pad_sequences(sequences, maxlen=100, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 99, 20)            273140    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 99, 50)            9200      \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 50)                15200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 13657)             696507    \n",
      "=================================================================\n",
      "Total params: 994,047\n",
      "Trainable params: 994,047\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 3357s - loss: 6.1366 - accuracy: 0.0572\n",
      "Epoch 2/50\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 20, input_length=max_length-1))\n",
    "#model.add(LSTM(50))\n",
    "model.add(Bidirectional(LSTM(25, return_sequences = True)))\n",
    "model.add(Bidirectional(LSTM(25, return_sequences = False)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=50, verbose=2)\n",
    "# evaluate model\n",
    "#print(generate_seq(model, tokenizer, max_length-1, 'Jack', 4))\n",
    "#print(generate_seq(model, tokenizer, max_length-1, 'Jill', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack and Jill went up the hill to\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sullivan called on CPS correct improve employee accountability example\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq(model, tokenizer, max_length-1, 'Sullivan called on CPS', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1,\n",
       " 'jack': 2,\n",
       " 'jill': 3,\n",
       " 'went': 4,\n",
       " 'up': 5,\n",
       " 'the': 6,\n",
       " 'hill': 7,\n",
       " 'to': 8,\n",
       " 'fetch': 9,\n",
       " 'a': 10,\n",
       " 'pail': 11,\n",
       " 'of': 12,\n",
       " 'water': 13,\n",
       " 'fell': 14,\n",
       " 'down': 15,\n",
       " 'broke': 16,\n",
       " 'his': 17,\n",
       " 'crown': 18,\n",
       " 'came': 19,\n",
       " 'tumbling': 20,\n",
       " 'after': 21}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "I was just trying to hit it hard someplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_pred_sim.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model,'word_pred_sim.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(tokenizer,'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer1 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras_preprocessing.text.Tokenizer"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'and',\n",
       " 2: 'jack',\n",
       " 3: 'jill',\n",
       " 4: 'went',\n",
       " 5: 'up',\n",
       " 6: 'the',\n",
       " 7: 'hill',\n",
       " 8: 'to',\n",
       " 9: 'fetch',\n",
       " 10: 'a',\n",
       " 11: 'pail',\n",
       " 12: 'of',\n",
       " 13: 'water',\n",
       " 14: 'fell',\n",
       " 15: 'down',\n",
       " 16: 'broke',\n",
       " 17: 'his',\n",
       " 18: 'crown',\n",
       " 19: 'came',\n",
       " 20: 'tumbling',\n",
       " 21: 'after'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
